# -*- coding: utf-8 -*-
"""IST_736_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D3ZRWHvJJE7cFi10kO5K2BHB-HvZNEK_

## Loading the data into the environment (once decided:)
"""

import os

from google.colab import drive
drive.mount('/content/drive')

os.chdir('/content/drive/My Drive/Python/IST 736/Final')

import pandas as pd
df = pd.read_csv('mbti_1.csv')

df.head()



"""## Maryse Khoury Section:"""

"""
PROJECT - Naive Bayes Model

Maryse Khoury

"""
import nltk
import pandas as pd
import pandas
import sklearn
import re  
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as dsw
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
## For Stemming
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
import os
from nltk import word_tokenize          
from nltk.stem import WordNetLemmatizer 
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
import string
import numpy as np
import requests


mbtiFileName = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/mbti_1.csv"
mbtiFILE = open(mbtiFileName,"r", encoding='latin1') 
    

ENFJPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/ENFJ"
IsFolderThere=os.path.isdir(ENFJPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(ENFJPath)
    
ENFPPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/ENFP"
IsFolderThere=os.path.isdir(ENFPPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(ENFPPath)
    
ENTJPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/ENTJ"
IsFolderThere=os.path.isdir(ENTJPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(ENTJPath)
    
ENTPPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/ENTP"
IsFolderThere=os.path.isdir(ENTPPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(ENTPPath)
    
ESFJPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/ESFJ"
IsFolderThere=os.path.isdir(ESFJPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(ESFJPath)
    
ESFPPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/ESFP"
IsFolderThere=os.path.isdir(ESFPPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(ESFPPath)
    
ESTJPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/ESTJ"
IsFolderThere=os.path.isdir(ESTJPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(ESTJPath)
    
ESTPPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/ESTP"
IsFolderThere=os.path.isdir(ESTPPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(ESTPPath)
    
INFJPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/INFJ"
IsFolderThere=os.path.isdir(INFJPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(INFJPath)  
    
INFPPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/INFP"
IsFolderThere=os.path.isdir(INFPPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(INFPPath)
    
INTJPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/INTJ"
IsFolderThere=os.path.isdir(INTJPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(INTJPath)      
    
INTPPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/INTP"
IsFolderThere=os.path.isdir(INTPPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(INTPPath)  
    
ISFJPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/ISFJ"
IsFolderThere=os.path.isdir(ISFJPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(ISFJPath)   
    
ISFPPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/ISFP"
IsFolderThere=os.path.isdir(ISFPPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(ISFPPath) 
    
ISTJPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/ISTJ"
IsFolderThere=os.path.isdir(ISTJPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(ISTJPath)
    
ISTPPath = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/ISTP"
IsFolderThere=os.path.isdir(ISTPPath)
print(not(IsFolderThere))
if (not(IsFolderThere)):
    MyNewCorpus=os.makedirs(ISTPPath)



mbtiFILE.seek(0)
for row in mbtiFILE:
    mRawRow = "The next row is: \n" + row +"\n"
    print(mRawRow)

mbtiFILE.seek(0)
Counter = -1
for row in mbtiFILE:
    RawRow = "The next row is: \n" + row +"\n"
    print(RawRow)
    NextRow = row.strip()
    NextRow = NextRow.strip("\n")
    NextRow = NextRow.strip("\(-)!*|:;")
    NextRow = NextRow.rstrip(",")
    MyList_mbti = NextRow.split(",")
    #print(MyList_mbti)
    My_Blank_Filter = filter(lambda x: x != '', MyList_mbti)
    
    MyList_mbti = list(My_Blank_Filter)  
    TheLabel = MyList_mbti[0]
    print(TheLabel)
    MyList_mbti.pop(0)
    Counter = Counter+1               
    if(Counter >= 1):
        NewFileName = TheLabel+str(Counter)+".txt"
        print(NewFileName)
        if (TheLabel == "ENFJ"):
              NewFilePath = ENFJPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close()
        elif (TheLabel == "ENFP"):
              NewFilePath =ENFPPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close()
        elif (TheLabel == "ENTJ"):
              NewFilePath =ENTJPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close()
        elif (TheLabel == "ENTP"):
              NewFilePath =ENTPPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close() 
        elif (TheLabel == "ESFJ"):
              NewFilePath =ESFJPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close()
        elif (TheLabel == "ESFP"):
              NewFilePath =ESFPPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close()
        elif (TheLabel == "ESTJ"):
              NewFilePath =ESTJPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close()
        elif (TheLabel == "ESTP"):
              NewFilePath =ESTPPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close() 
        elif (TheLabel == "INFJ"):
              NewFilePath =INFJPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close()
        elif (TheLabel == "INFP"):
              NewFilePath =INFPPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close()
        elif (TheLabel == "INTJ"):
              NewFilePath =INTJPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close()
        elif (TheLabel == "INTP"):
              NewFilePath =INTPPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close()
        elif (TheLabel == "ISFJ"):
              NewFilePath =ISFJPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close()
        elif (TheLabel == "ISFP"):
              NewFilePath =ISFPPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close()
        elif (TheLabel == "ISTJ"):
              NewFilePath =ISTJPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close()
        elif (TheLabel == "ISTP"):
              NewFilePath =ISTPPath+"/"+NewFileName
              n_file = open(NewFilePath, "w", errors="ignore")
              MyNewString = " ".join(MyList_mbti)
              MyNewString.maketrans('', '', string.punctuation)
              n_file.write(MyNewString)
              n_file.close() 
mbtiFILE.close()

print(ENFJPath)
print(os.listdir(ENFJPath))

print(ENFPPath)
print(os.listdir(ENFPPath))   
    
print(ENTJPath)
print(os.listdir(ENTJPath))

print(ENTPPath)
print(os.listdir(ENTPPath))
    
print(ESFJPath)
print(os.listdir(ESFJPath))

print(ESFPPath)
print(os.listdir(ESFPPath))   

print(ESTJPath)
print(os.listdir(ESTJPath))

print(ESTPPath)
print(os.listdir(ESTPPath))
    
print(INFJPath)
print(os.listdir(INFJPath))

print(INFPPath)
print(os.listdir(INFPPath))   

print(INTJPath)
print(os.listdir(INTJPath))

print(INTPPath)
print(os.listdir(INTPPath))   
    
print(ISFJPath)
print(os.listdir(ISFJPath))

print(ISFPPath)
print(os.listdir(ISFPPath))   

print(ISTJPath)
print(os.listdir(ISTJPath))

print(ISTPPath)
print(os.listdir(ISTPPath)) 
    

MyStops = ['aa','aaaaaaaaa','aaaaaaaaaa','aaaaaaaaaes','aaaaaaaaagw','aaaaaaaaali','aaaaaaaaar','aaaaaaaao','aaaaiiiii','aaaand','aaand','aac','aages','aah','aaj','aal','aand','aang','aao','aaqskzjrgabaqaaaqabaad','aas','ab','abe','abf','abt','abyss','ac','acf','adk','ae','aeb','aelthwyn','afaik','afiak','afoot','afore','afqjcng','agk','agwc','agyeman','ah','aha','ahaaa','ahah','ahaha','ahahah','ahahaha','ahahahaha','ahahahahahahahaha','ahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahaahhahaha','ahdmns','ahe','ahem','ahh','ahha','ahhaha','ahhh','ahhhh','ahhhhh','ahhhhhhhhh','ahhhhhhhhhhhhhhhh','ahlicks','ahm','ahr','ahruai','ai','aight','aioli','ajpau','ajripdpgpfa','ajvnji','ak','aka','akala','akamaihd','akdunijlugm','akfqg','akira','akito','aklways','al','alamo','alan','alanis','albom','alec','algq','alllll','allllllllllllllll','alllllways','allllways','allong','alwaaayssss','ambivertnfj','ands','anon','ap','apyxku','arggghhh','asxdvlpw','athf','atiw','aw','awb','aweee','aweosum','awgf','awh','aww','awww','awwwww','awwwwwwww','awwwwwwwwwwwwwwwwwwwwwwwwwwwwwww','ax','aybgh','aygnfbb','ayt','ayup','azarth','b','ba','backk','badddddd','bae','bah','bahaha','bahahahahahaha','bb','bdbpzjbxuzi','bdovg','beinga','bev','bfb','bfcjpz','bh','bi','bias','bic','bikbxqzcaaasnd','bjbrjk','bjorn','bkeg','bkysjcs','blah','blahh','blahhhhhhhhhhhhhhh','blf','blye','bn','bnd','bnkfvig','bno','boa','bowieownsmysoul','bt','buuuuut','bwxazvhlyxq','bxyqyz','byrbxxvs','byw','bzf','caaall','cao','cb','cbb','cbbtvlq','cbd','cbie','cbk','cbt','cca','ccc','ccngyhfsv','cda','cdn','cds','cdtfdpahbni','cdxhao','cf','cfg','cftvvdrboi','chickpeaax','chzbgr','cm','cme','cnd','cnfrg','cos','cosa','cpxnisk','cq','crfmvfw','cri','crq','cuuutee','cuz','cvopgyygc','dasg','dbd','dbt','dcaed','dcdab','dd','ddd','dddd','dddf','dde','ddnyf','defo','deokdyc','dga','dgs','dhs','djq','djux','dk','dlzooqhazgc','dm','dmb','dna','dnh','dnyf','dodkeu','doe','doer','doesnt','doh','doml','doms','doqlxrt','downton','dowwwwwwn','doyly','dp','dphtcu','dqdo','dqe','dqfa','duh','duhhhhhhhhhhhhhhhhhhhhhh','dvgqyo','dvnq','dvrcykzhy','dvvvii','dwell','dwylnbw','dysg','ealq','eb','eba','ebd','ebil','ec','eca','ecf','ee','eeeep','eek','eep','eerily','eew','ef','eff','eguwgtyo','egvvqsung','eh','ehehehe','ehhh','ehm','eiag','eichen','eie','eilzug','einstein','eked','ekstatische','ektodua','elew','eli','emaepzsxr','emehxa','ere','esfplolitslieksooooobviouslerl','eske','esme','esohomkwjeg','esp','estj','estjs','estp','estps','esuo','esxj','esxp','eta','ethuzjiwx','etq','evah','evarr','evgn','ew','ewell','ewhguaowy','ewww','exintj','eyre','eyrha','eyup','f','fa','fae','fba','fbcdn','fc','fded','fdjnz','fe','feqewf','fey','ffs','ffximage','fg','fiyeeeeeeerrrrrrrroooooooo','fiyero','fj','fjaydr','fjcdn','fjords','fko','fl','fuuxxkkrpq','fv','fwatch','fwb','fwiw','fwww','fy','fyi','fyzuypxg','g','gah','gahhh','gahhhh','gbxa','gfbbc','gfto','gg','gggvnq','gkp','gustav','gv','gvt','gw','gwailo','gwen','gwulcih','gzc','h','ha','haaaa','haaaaaaaaaaaa','hadn','hah','haha','hahaa','hahah','hahaha','hahahaa','hahahaaa','hahahaahahaa','hahahah','hahahaha','hahahahah','hahahahahah','hahahahahaha','hahahahahahaha','hahahahahahhaha','hahahahhahahahaha','hahahha','hahha','hahhaha','hallao','han','haz','hed','heeellll','heeheehee','heh','hehe','hehehe','heheheheh','heimlich','heinlein','hella','helllooo','hellooooooo','helloquizzy','herheinie','hg','hgwd','hgy','hhrnseuvzlm','highclasssavage','hihi','hii','hiii','hiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii','hiya','hjwi','hkcijtgg','hkdxvvom','hlvbg','hm','hmhkim','hmm','hmmm','hmmmm','hmmmmm','hmmmmmm','hnmmm','hnnng','hnp','homelyroamlydomely','hphotos','hqii','huh','hulk','hum','hunza','huuh','huuhugghugghughgug','huzzah','hwytatf','hydra','hype','hyper','hypothesis','hyuk','iagwu','ibafbd','ih','ihkq','iin','iisu','ijihdmyhe','ijqi','ikbjxzf','ikopvm','ildfp','ili','ipskjxk','ista','istj','istjs','istp','istps','istx','isuppose','isxjs','isxp','italian','italy','ive','ivy','ix','ixfj','ixfp','ixfx','ixtj','ixtjs','ixtp','ixxj','ixxx','iz','izayoi','j','jp','jpg','jq','jr','jrv','js','ju','jydu','jyn','jyyekhk','jz','k','khiro','kitteh','kitty','kittykraz','kja','kjo','kjvde','kk','kkking','kl','klimt','klinga','klk','klok','knack','kne','ko','kobe','koda','kony','kore','kpfqgqali','kpmkb','kpop','kqem','krxnj','ktd','kti','kurumu','kuu','kvg','kvteh','kw','kwttdpgkgs','kx','kxy','ky','kyle','kyusaku','kz','l','laaawl','ldj','ldjy','ldr','ldu','littlemisssith','liv','ljlsi','lkza','llsvrzn','lmao','lofzlb','loki','loking','lol','loling','loll','lolll','lolol','lolw','loooonely','loooooooong','looooooooooool','looooooooooooooooooooooooooool','looooove','loooove','loove','lotr','lotsa','loveeeeee','lovveee','lovvvve','lp','lpcu','lplaff','lpmmkjy','lptozrk','lpvnxdl','lpwgs','lpyezrfdmy','lpypw','lpyq','lpyr','lq','lqjwlzqilxk','lqlihraql','lr','lrg','ls','lsd','lsho','lsi','lssqcb','lsu','ltr','lu','lurver','lust','luuuuuuuuuuuuuuv','lvbuckeye','lvl','lw','lwgn','lwpeg','lwqgztda','lwslzlomuj','m','ma','maaya','mawc','mayyyybe','mbqx','mch','meee','meeehhh','meeoooowwwww','meeps','meh','mei','mel','melancholic','melancholics','melancholy','melange','melatonin','melchiz','meld','melissa','mellow','melodeath','melody','melt','melted','melts','member','members','meme','mew','mg','mhhhhhhhhh','mho','mj','mjxthwl','mm','mma','mmm','mmmaybe','mmmh','mmmhmm','mmmm','mmmmhm','mmmmm','mmmmmmmmm','mnimp','mnt','mo','mow','mp','mph','mplg','mpuyqc','mr','mrgreendots','mrs','ms','msbossypants','msot','mssdnaalqc','mtfthffbtds','mtl','mtv','muahahahahahaha','muahhahahahahha','muay','mucha','muf','mwahaha','mwtocre','myelin','myers','myfacewhen','myjazz','myjtrwomso','mystikro','mythical','myths','myxbiy','naan','nbd','nbpcaartw','nbxghazuwb','nc','nckg','nd','ndsj','nfujwzqg','ng','nhqyljzs','niccolo','njchcik','njchick','nlbeizbvq','nmpxjio','nmzaukgw','nn','nnc','nne','nnjrbuzxdjk','nnnnngh','nnnnnnhhhjuaaaaaaaah','nooo','noooo','nooooo','nooooooooo','noooooooooo','nooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo','noooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo','nope','nox','nps','npz','nq','nr','ns','nt','ntj','ntjs','ntp','ntps','nts','ntuqtuc','nutellllllaaaaaaaa','nuuuuuuuuuuuuuuuuuuuuuuuuuuuu','nuxkdo','nvbh','nvm','nwnu','nwrl','nxigcmce','nxnxtniwkc','ocurse','od','odej','ogdmo','oh','ohgod','ohh','ohhh','ohhhh','ohhhhh','ohhhhhhhh','ohthosetwinks','oin','oj','ojetibembjy','ojja','ojwje','ok','oka','okay','okc','okcupid','okey','okkultisme','oklahoma','okpqm','ol','olaf','old','older','oldest','oldman','olds','ole','oleas','olenna','oligodendrocytes','ollllllllld','ombre','omegj','omfg','omgggggg','omgoodness','onfz','onitieellt','onn','ood','ooey','oogle','ooh','oonly','ooo','oooh','oooo','ooooh','ooookay','oooooo','ooooooh','ooooooo','oops','oopsy','oor','op','ovzq','ow','owwwww','oy','oz','ozsjxu','p','pa','ply','pm','pmed','pmj','pml','pmsisawesomenotreally','pneumoceptor','png','po','pvzvqk','pxufi','pyn','pyoq','pywni','q','qaacvsgramo','qbhxibo','qbsnqxo','qbtitco','qbye','qbzzgco','qcdta','qcipq','qcrp','qdslhxo','qdzczwo','qeboqr','qfbuiwo','qfi','qflhgq','qfnpmni','qg','qgahahmy','qgib','qgs','qjo','qk','ql','qlf','qlq','qmdugbo','qmokmq','qmqz','qmxbujm','qnkq','qolsfu','qosnm','qpeafanes','qrodxw','qrzj','quow','quuc','qvib','qw','qwoy','qx','qxkykq','qxm','qxw','qxynpuuuct','qzbxoeo','qziy','qzzelmo','r','ra','raania','repatzdqiygim','repc','rfzt','rhapsody','rhbxjcq','rjo','rk','rl','rlw','rm','rmc','rmva','rmxmg','rnb','rnw','ro','roaaaar','rpg','rpgs','rq','rrsgedfjwo','rtm','ru','rv','rwppytmfi','rx','ryukishi','s','scuei','scum','sdawkminn','se','shhhhhhhh','shortaaa','sigur','siiiiiiiiigh','sil','slmeyhinf','soo','soon','sooner','sooo','soooo','sooooo','soooooo','speeeeeeeeeeeeeeeeeeeeeeddd','sriracha','srisvjxi','stfu','sti','sx','sxt','sy','syd','syfy','szb','szbein','t','ta','tbf','tbh','tbn','tbzrqgnuim','tcdns','tch','tdxylqhu','te','thguest','tiy','tj','tjs','tl','tmi','tmlt','tnt','ts','tsk','tsm','tsun','tt','tuowzxkkbq','tvd','twat','tyrion','tywin','tywog','tz','tzu','u','ubr','ubzramrxy','uc','ucg','uck','ucl','ucuralvim','uddhanbbce','udf','uds','uf','ufbdv','ufeffthe','uff','ufff','ufi','ufplsog','ug','ugg','ugggh','uggh','ugh','ughhh','ughhhh','ugkg','uglidy','ugy','uh','uhfq','uhh','uhhanybody','uhhh','uhhhh','uhhhhhhhhh','uhjg','uhm','uhpeey','uhsvvpj','uhwo','ui','uji','uk','ukp','ukraine','ukulele','ultranumb','ulysses','um','umacdfyzw','umdr','umm','ummm','ummmm','ummmmm','ummon','uqa','uqk','urgh','uuunwvelyq','uvnt','uwgu','uwi','uwin','uxjr','uxqh','uy','uyio','uyog','uzw','v','vc','ve','vee','vel','vf','vftu','vfu','vgys','vi','viabi','vizzini','vkpco','vksndp','vmvmrssst','vngogu','vo','vof','vr','vrz','vs','vsd','vsgl','vsvr','vtpi','vu','vze','vziviacmsu','w','wa','waaaaaay','waaaay','waaaayyyy','waaay','waayyy','waffles','wager','wahhahahaa','wahhhhhh','waow','wayupnorth','wayyy','wayyyyy','wbldcc','wcndk','wcrznzqm','wcwjv','wd','wdw','weebl','weeeeeeeeeeeeeeeeee','weehooo','weknowmemes','wfh','wfxy','wfyz','wg','wh','whaaaaa','whaaat','whahhh','whatx','whhecugi','whhhaaaatttttt','whicdn','whoo','whooooooole','wi','wjes','wk','wlak','wlb','wlklfc','wlzeakkc','wmdvg','wnoztq','wnqfngiw','woamnljl','wobbly','woho','wohoo','woooo','wooooooooooooooooooooow','worf','wrhfovywata','wrpg','wrpgs','wrsfjhlx','wrt','ws','wsadelz','wsi','wsj','wsmjf','wswiki','wtf','wtfe','wtvr','wtwgdfrc','wu','wud','wulfheart','wulyrnp','wuo','wuss','wut','wuthering','wv','wvhaqx','wvigy','wvpje','ww','wwii','wwoof','wwpqqta','www','wwwwwwwwwwwwwwwwwat','wx','wxjtou','wxyz','wxzpl','wyatt','wyd','wyk','wyldstyle','wyljvkog','wyndam','wytch','wzpmvzde','wzzyxmkaq','x','xagkdkf','xanax','xbvwnaq','xcfek','xchaebr','xd','xdd','xdo','xe','xesx','xfkzrnyygfk','xfnj','xfo','xfq','xft','xhc','xhmjf','xkiszwp','xkmtngm','xl','xld','xlillithx','xlwsfh','xmap','xmas','xnfj','xnfjs','xnfp','xnfps','xnfx','xnfxs','xntaqnsyrlckbcqokdgwofw','xntj','xntjs','xntp','xntps','xntx','xntxs','xnwfw','xnxp','xo','xodmwwnv','xog','xorsyst','xoxoxoxixhks','xp','xpokjmdgm','xqj','xreanq','xrgq','xrgrzf','xrys','xs','xsfj','xsfjs','xsfp','xshq','xsmall','xstj','xstjs','xstp','xstps','xstx','xsxp','xt','xtmpv','xuoaae','xvk','xwbu','xwl','xww','xx','xxfj','xxfjs','xxfps','xxjnug','xxrm','xxtp','xxx','xxxj','xxxp','xxxx','xxxxx','xxxxxxxxxx','xxxxxxxxxxxxxxxxxxx','xy','xz','y','ya','yaaaaaaaaas','yaaay','yaay','yaears','yahh','yahu','yak','yao','yard','yas','yawwwwwnnnn','yawwwwwwwwwn','yax','yayy','yc','yctgryglsqy','ydpxh','ye','yea','yeaa','yeaaa','yeaaaah','yeaaah','yeaah','yeah','yeahhhhhh','yeczkgfxzzg','yeehaw','yeetfzg','yeh','yehh','yehhhh','yelkin','yeoreum','yess','yesss','yessss','yessssss','yeu','yey','yezi','yfjtzlxekig','yg','yghxpay','ygkmtlane','ygritte','ygro','ygsx','yh','yhv','yhzetgs','yi','yiagm','yiannopoulos','yield','yin','yix','ykegtwjuzis','ykh','ylbnj','yle','ylfakwwwt','ylfv','ymd','ymh','yndorsmaclsm','yo','yoda','yodr','yolo','yolw','yooooooooooooooooooooooooooork','yooyoung','yoshimoto','youded','youll','young','younge','younger','youngest','youre','yourebffjill','yourlogicalfallacyis','yousafzai','youth','youtu','youtube','youtubers','youviolate','yowch','yp','ypk','yq','yqambq','yr','yrmtodi','yrqob','ys','yslpw','ysupy','yt','ytb','ytimg','ytvg','ytxjp','yuck','yucky','yugi','yugioh','yuhi','yukari','yum','yummmm','yummy','yup','yupyyollfo','yuri','yuujinchou','yv','yvkgu','ywzb','yx','yy','yz','yzge','yzx','z','zada','zaknafein','zayn','zbay','zbgeytsji','zbzu','zdndjf','zduq','zdwdqbdbd','ze','zeqe','zey','zgiz','zgk','zh','zhvbxca','ziauddin','zim','zjbckrjq','zk','zkey','zkidjjy','zm','zmz','zoe','zoidberg','zojmb','zolb','zoloft','zombie','zombies','zomfgum','zoog','zorro','zowie','zp','zpalxewctku','zpdqjni','zps','zpsb','zpsda','zpsipjhn','zpsqhtljhyg','zpsthyc','zpuyjpkg','zqa','zqt','zqtiqv','zraqmkuki','zrbhg','zs','zscd','zsefrum','zsipudig','zsj','zska','zslb','zsyq','ztdh','ztpi','ztqyj','zu','zuatjtr','zubs','zuecmu','zujava','zuko','zulban','zulnex','zuly','zuo','zuodekx','zuxfun','zvda','zvfptdwgfy','zvls','zvq','zvxlrwzex','zwc','zweig','zwfggc','zwmrfgj','zwow','zwrz','zx','zxad','zxbt','zxbtch','zxdusmca','zxdwi','zxhgopie','zxhuso','zxia','zxiokqe','zxjvnm','zxktfknft','zxlwg','zxpvgbu','zxsxhyv','zxtqu','zxut','zxvjinymzqy','zxwcbic','zxx','zxy','zxzsb','zxzzr','zy','zybyuih','zycv','zydopg','zydrate','zyf','zyg','zygon','zyhx','zyjl','zyjov','zyjsncqioq','zykw','zylczg','zylh','zylx','zyn','zyng','zynk','zynthax','zynthaxx','zyoxonws','zypae','zyq','zytdg','zytghs','zyuaehfc','zyucgi','zyuw','zyvknyl','zywcmcb','zyxwvutsrqponmlkjihgfedcba','zyy','zyzz','zz','zzaib','zzaprng','zzbrandon','zzby','zzc','zzckd','zzdykmqaqno','zzf','zzfauscrxcc','zzfqwxeqyai','zzga','zzgcdq','zzgopwuy','zzh','zzhgrq','zzhk','zzkbdoge','zzkm','zzko','zzkuzrjuzq','zzlbsnb','zzlvav','zzm','zzn','zzpbl','zzqkcneo','zzqs','zzr','zztr','zztxxpgcjfk','zzu','zzufagmlao','zzvg','zzvs','zzvuhub','zzw','zzward','zzwutjic','zzxkgxc','zzyfcys','zzz','zzzquil','zzzr','zzzvader','zzzy','zzzz','zzzzero','zzzzz','zzzzzzzz','zzzzzzzzzzz','zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz','zvkzbuugr', 'zvsglhwr', 'zvtwvwqbzm', 'zvvmltyeq', 'zw', 'zwa','zwanglos', 'zwbw', 'zwwc', 'zylinder','yakumo', 'yaomi', 'yeaaaa', 'yeahright', 'yesteday', 'ypn','wikimg','yeess', 'yondu','xtbo','yardiff','yeeeeeaaaaahhhhhhh', 'yesssssss', 'yoshitaka', 'zan','zvkn', 'zvljiihs', 'zvwjfkm', 'zwamouyrnbg', 'zwnmjgc', 'zwpc','zxjwo', 'zxo','zgkymdezlza', 'zhuangzi', 'zika', 'zippy', 'ziyenrbbkbw', 'zjk','zkio', 'zlbq', 'zolphc', 'zvkzbuugr', 'zvsglhwr', 'zvtwvwqbzm', 'zvvmltyeq', 'zw', 'zwa','zwanglos','zwbw', 'zwwc','zoobicknizer','ztsu','zuhvpnv', 'zuieav', 'zukg', 'zun','zunga', 'zuul', 'zv','yxz', 'yzy', 'zcnysau', 'zdkk','zeitgeist','zerkalo', 'zerooooo','zrzmhu', 'zsh', 'zt', 'zti', 'ztommi','ztyckcezu', 'zullen', 'zuqk','willmost', 'winchie', 'wko', 'woud', 'wreaths', 'wwqi', 'xenomorh','wht', 'woodworks', 'wooooow', 'worstbreakuplinesalltime','zvtrh', 'zwangos', 'zwgsx', 'zwgwrrilz', 'zwjm', 'zwq', 'zxdo','zxjlaowf', 'zxk', 'zxvheazh','zxwvw', 'zygotes', 'zyhr', 'zyhu', 'zynga', 'zypkh', 'zyswyijdtpg','zyu', 'zywcr', 'zza','zvoxgxlasac', 'zvwb', 'zvz', 'zwn', 'zwoeowmhg', 'zwxim', 'zwxs','zwzy', 'zxkbd', 'zxs','zxgu', 'zxlvisd', 'zxp', 'zxugx', 'zxuklu', 'zydhg', 'zygomorphic','zygote', 'zyhryis', 'zylapvi','zjqptmjmhbnn', 'zkqdy', 'zlw', 'znw', 'znz', 'zodmtnygtlisbcgokdg','zpt', 'zqjeid', 'zqzy', 'zsgydm','zlsk', 'zlyszaunyfg', 'zoem', 'zom', 'zqxngcm', 'zrnrled', 'zrssfw','zstspukt', 'ztnjmugrzrmrajg', 'ztthespmdoy','zjpmdmo', 'zkzmexy', 'zlwa', 'zmjmawaayh', 'zpp', 'zppjcqaqxyo','zpsdwvww', 'zqnfmzoi', 'zqplpqaq', 'zqyb','zpzsx', 'zqkz', 'zqmi', 'zqp', 'zsbhadm', 'aaaaaaaajhe', 'zttjqb', 'zuphm', 'zvdblbz', 'zvefpmxo', 'aaaaand', 'zvn', 'aqualad', 'ntt', 'abhkvubq', 'acb', 'aaaaahhhhahahahahahaha', 'aaaaaaaatqg', 'aab', 'aaaaaand', 'aafa', 'aab', 'aaaanyways', 'abd', 'abamro', 'aaaaaaaazsa', 'aaaan', 'aaaaaaaajhe', 'ndy', 'ovmbwe', 'https',' http', 'dbz','nan', 'nanajndasdsadas', 'null', 'itydjt', 'es','zr', 'zrdsvdh', 'zrirw', 'zsbjc', 'zsg', 'zskex', 'zsnwym', 'zster','yus', 'yusi', 'yuugi', 'yuuri', 'yuuriu', 'yves', 'yvo', 'zpgecamfttq', 'zppt', 'zpsympwlvjh', 'zpyfazhazy', 'zqwpjngduiq','zqxk', 'zrat', 'zrbhgia', 'zato','xthecaramelqueenx','wpc','zue', 'zuicker', 'zukav', 'zukf', 'zupimages', 'zusak', 'zuxn','zvaijl', 'zvcv', 'zvpsqk','zwu', 'zwyi', 'zxbd', 'zxirryyrgnr', 'zxnfd', 'zxnqntfw', 'zxpozo','zxsqaxx', 'zxveffe', 'zxvzzticvjs','zuqiufz', 'zurhvy', 'zusammenhangend', 'zusnici', 'zuto', 'zuuobgsb','zuvk', 'zuw', 'zvaxa', 'zvoc''zvm', 'zvoomlc', 'zvuyyhykbhk', 'zvwcdnkpkpe', 'zvzq', 'zwkbfcy','zwp', 'zww', 'zxcqpik', 'zxg','zazzy', 'zbw', 'zdgqb', 'zekmk', 'zelo', 'zgi', 'zgo','zhltdr', 'zio','zaps', 'zazzy', 'zbw', 'zdgqb', 'zekmk', 'zelo', 'zgi', 'zgo','zhltdr', 'zionx\'nx','zionx','nx''zgtnrujzvbu', 'zhax', 'zhjya', 'zhzxex', 'zigibw', 'zilla', 'ziop','zjozc', 'zjzjmugx', 'zkh','yylkifz', 'zarathoustra', 'zazi', 'zcaygk', 'zdjndawmig', 'zevon','zgb', 'zgggre', 'zjgxndhkytqwmdq', 'zjh' 'zlvqqnub', 'zlwlg', 'zmn', 'zmza', 'znzwuslkz', 'zojkf', 'zoneshot','zord', 'zoxp', 'zoywc','zioelk', 'zipfoo', 'ziuk', 'zjlrtt', 'zkjsqw', 'zklaoksixuy','zlhmuapgqs', 'zlnhpe', 'zlo', 'zlvqqnub','ytnmblfbptc', 'ytwl', 'ytx', 'yuca', 'yull', 'yvec', 'yvyunpl', 'ywm','yxl', 'zjh','zddhew', 'zdjms', 'zetokfqjg', 'zexy', 'zfk', 'zfond', 'zga', 'zgix','zgtg', 'zgtnrujzvbu','yuacxx', 'yuksek', 'yummmmmmmmmmmmmmmmmmmmy', 'ywdhycstsui', 'yxt','yxta', 'yydmetmjpdu', 'yzgjdbm', 'zanimus', 'zaoq''zum', 'zure', 'zus', 'zutm', 'zuwut', 'zva', 'zvbn', 'zvcamxws','zvgl', 'zvm','ztgi', 'ztjptbpnzhwbwcdnw', 'ztl', 'ztvmib', 'zubz', 'zufys','zuhblhmbs', 'zulhdsj', 'zumgsbwal', 'zvoc','zvo', 'zvwic', 'zvyxgi', 'zvzvmig', 'zwei', 'zwette', 'zwfens','zwitterion', 'zws', 'zwsnzahtoto','ztbrfawgbg', 'ztc', 'ztk', 'ztkuko', 'ztm', 'ztoavij', 'ztvc','ztwfiw', 'zlhmuapgqs','zlnhpe','zlo','zlvqqnub','oymhvchq','itx','dmt']


stop_words = text.ENGLISH_STOP_WORDS.union(MyStops)
#stop_words = frozenset(MyStops)
#print(stop_words)

'''
class LemmaTokenizer(object):
    def __init__(self):
        self.wnl = WordNetLemmatizer()
    def __call__(self, articles):
        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]

nltk.download('wordnet')

porter_stemmer = PorterStemmer()
from textblob import TextBlob

# Use TextBlob
def textblob_tokenizer(str_input):
    blob = TextBlob(str_input.lower())
    tokens = blob.words
    words = [token.stem() for token in tokens]
    return words

# Use NLTK's PorterStemmer
def stemming_tokenizer(str_input):
    words = re.sub(r"[^A-Za-z0-9\-]", " ", str_input).lower().split()
    words = [porter_stemmer.stem(word) for word in words]
    return words

'''

## Vectorize the text data for MNB
CountVectMNB = CountVectorizer(input='filename',
                             analyzer = 'word',
                             #stop_words ='english',
                             strip_accents = 'ascii',
                             #tokenizer=textblob_tokenizer, #LemmaTokenizer(),
                             stop_words = stop_words,
                             token_pattern='(?u)[a-zA-Z]+',
                             #r"\w+", #'(?u)r[^\w\s a-zA-Z]+',
                             max_features = 1000000,
                             decode_error = 'ignore'
                             )


#Create a new empty data frame for MNB 
FinalmbtiDF = pd.DataFrame()


for R in ["ENFJ", "ENFP", "ENTJ", "ENTP", "ESFJ", "ESFP", "ESTJ", "ESTP", "INFJ", "INFP", "INTJ", "INTP", "ISFJ", "ISFP", "ISTJ", "ISTP"]:
    builder = R + "DF"
    print(builder)
    path = "C:\\Users\\GORGEOUS!\\Desktop\\2020-0405 IST Text Mining\\Project\\" + R
    
    FileList=[]
    for item in os.listdir(path):
        #print(path+ "\\" + item)
        next= path + "\\" + item
        FileList.append(next)  
        #print(FileList)
        
        X_MNB = CountVectMNB.fit_transform(FileList)
        ColumnNamesM = CountVectMNB.get_feature_names()
        #print("Column names: ", ColumnNamesM)
    #print(builder)   
    builder = pd.DataFrame(X_MNB.toarray(),columns = ColumnNamesM)
   
    ## Add column
    #print("Adding new column....")
    builder["Label"] = R
    #print(builder)
                  
    FinalmbtiDF = FinalmbtiDF.append(builder)
    print("\n\n Count Vect")
    print(FinalmbtiDF)
    print(FinalmbtiDF.columns)
   

# Remove words no in dictionary file
FinalmbtiDF1 = FinalmbtiDF

DICTIONARY = "C:/Users/GORGEOUS!/Desktop/2020-0405 IST Text Mining/Project/Google-10000-english-20k.txt"
with open(DICTIONARY) as DICT:
    words = set(line.strip() for line in DICT)
    #print(words)
    cols1 = [c for c in FinalmbtiDF1.columns if c.lower()[:] in words]
    FinalmbtiDF1 = FinalmbtiDF1[cols1]

    
list(FinalmbtiDF1.columns)
print("\n\n Remove words not in dictionary  from MNB DF")
FinalmbtiDF1.head()

## Replace the NaN witH 0
FinalmbtiDF1 = FinalmbtiDF1.fillna(0)
print("Normal DF Freq: ")  
print(FinalmbtiDF1)

  
############# Create Train/Test for MNB  
from sklearn.model_selection import train_test_split
## Create Train/Test for TrainDF
TrainDF, TestDF = train_test_split(FinalmbtiDF1, test_size=0.2)
print(TrainDF.head())

print("\nThe training set is:")
print(TrainDF)
print("\nThe testing set is:")
print(TestDF)

## Save labels
TestLabels=TestDF["Label"]
print(TestLabels)
## remove labels
TestDF = TestDF.drop(["Label"], axis=1)
print(TestDF)

TrainLabels=TrainDF["Label"]
print(TrainLabels)
TrainDF_nolabels=TrainDF.drop(["Label"], axis=1)
print(TrainDF_nolabels) 


########################### Naive Bayes ############################
    
from sklearn.naive_bayes import MultinomialNB
mbtiModelMNB = MultinomialNB()
mbtiModelMNB.fit(TrainDF_nolabels, TrainLabels)

## Prediction
Prediction = mbtiModelMNB.predict(TestDF)
print("\nThe prediction from MNB is:")
print(Prediction)
print("\nThe actual labels are:")
print(TestLabels)

## confusion matrix
from sklearn.metrics import confusion_matrix
cnf_matrix = confusion_matrix(TestLabels, Prediction)
print("\nThe MNB confusion matrix is:")
print(cnf_matrix)

print("\nThe MNB prediction probabilities:")
print(np.round(mbtiModelMNB.predict_proba(TestDF),2))

from sklearn.metrics import accuracy_score
print("\nMNB Accuracy score: ", accuracy_score(TestLabels, Prediction))

from sklearn.metrics import classification_report  
print(classification_report(TestLabels, Prediction))


# Top 20 words for each personality type
ENFJ_class_prob_sorted = mbtiModelMNB.feature_log_prob_[0].argsort()
ENFP_class_prob_sorted = mbtiModelMNB.feature_log_prob_[1].argsort()
ENTJ_class_prob_sorted = mbtiModelMNB.feature_log_prob_[2].argsort()
ENTP_class_prob_sorted = mbtiModelMNB.feature_log_prob_[3].argsort()

ESFJ_class_prob_sorted = mbtiModelMNB.feature_log_prob_[4].argsort()
ESFP_class_prob_sorted = mbtiModelMNB.feature_log_prob_[5].argsort()
ESTJ_class_prob_sorted = mbtiModelMNB.feature_log_prob_[6].argsort()
ESTP_class_prob_sorted = mbtiModelMNB.feature_log_prob_[7].argsort()

INFJ_class_prob_sorted = mbtiModelMNB.feature_log_prob_[8].argsort()
INFP_class_prob_sorted = mbtiModelMNB.feature_log_prob_[9].argsort()
INTJ_class_prob_sorted = mbtiModelMNB.feature_log_prob_[10].argsort()
INTP_class_prob_sorted = mbtiModelMNB.feature_log_prob_[11].argsort()

ISFJ_class_prob_sorted = mbtiModelMNB.feature_log_prob_[12].argsort()
ISFP_class_prob_sorted = mbtiModelMNB.feature_log_prob_[13].argsort()
ISTJ_class_prob_sorted = mbtiModelMNB.feature_log_prob_[14].argsort()
ISTP_class_prob_sorted = mbtiModelMNB.feature_log_prob_[15].argsort()

#ISTP_class_prob_sorted = mbtiModelMNB.feature_log_prob_[15, :765].argsort()

print("\nTop 20 ENFJ Words: ")
print(np.take(CountVectMNB.get_feature_names(), ENFJ_class_prob_sorted[:20]))
ENFJwords = np.take(CountVectMNB.get_feature_names(), ENFJ_class_prob_sorted[:20])
ENFJwords
print("\nTop 20 ENFP Words: ")
print(np.take(CountVectMNB.get_feature_names(), ENFP_class_prob_sorted[:20]))
ENFPwords = np.take(CountVectMNB.get_feature_names(), ENFP_class_prob_sorted[:20])
ENFPwords
print("\nTop 20 ENTJ Words: ")
print(np.take(CountVectMNB.get_feature_names(), ENTJ_class_prob_sorted[:20]))
ENTJwords = np.take(CountVectMNB.get_feature_names(), ENTJ_class_prob_sorted[:20])
ENTJwords
print("\nTop 20 ENTP Words: ")
print(np.take(CountVectMNB.get_feature_names(), ENTJ_class_prob_sorted[:20]))
ENTPwords = np.take(CountVectMNB.get_feature_names(), ENTP_class_prob_sorted[:20])
ENTPwords


print("\nTop 20 ESFJ Words: ")
print(np.take(CountVectMNB.get_feature_names(), ESFJ_class_prob_sorted[:20]))
ESFJwords = np.take(CountVectMNB.get_feature_names(), ESFJ_class_prob_sorted[:20])
ESFJwords
print("\nTop 20 ESFP Words: ")
print(np.take(CountVectMNB.get_feature_names(), ESFP_class_prob_sorted[:20]))
ESFPwords = np.take(CountVectMNB.get_feature_names(), ESFP_class_prob_sorted[:20])
ESFPwords
print("\nTop 20 ESTJ Words: ")
print(np.take(CountVectMNB.get_feature_names(), ESTJ_class_prob_sorted[:20]))
ESTJwords = np.take(CountVectMNB.get_feature_names(), ESTJ_class_prob_sorted[:20])
ESTJwords
print("\nTop 20 ESTP Words: ")
print(np.take(CountVectMNB.get_feature_names(), ESTJ_class_prob_sorted[:20]))
ESTPwords = np.take(CountVectMNB.get_feature_names(), ESTJ_class_prob_sorted[:20])
ESTPwords


print("\nTop 20 INFJ Words: ")
print(np.take(CountVectMNB.get_feature_names(), INFJ_class_prob_sorted[:20]))
INFJwords = np.take(CountVectMNB.get_feature_names(), INFJ_class_prob_sorted[:20])
INFJwords
print("\nTop 20 INFP Words: ")
print(np.take(CountVectMNB.get_feature_names(), INFP_class_prob_sorted[:20]))
INFPwords = np.take(CountVectMNB.get_feature_names(), INFP_class_prob_sorted[:20])
INFPwords
print("\nTop 20 INTJ Words: ")
print(np.take(CountVectMNB.get_feature_names(), INTJ_class_prob_sorted[:20]))
INTJwords = np.take(CountVectMNB.get_feature_names(), INTJ_class_prob_sorted[:20])
INTJwords
print("\nTop 20 INTP Words: ")
print(np.take(CountVectMNB.get_feature_names(), INTJ_class_prob_sorted[:20]))
INTPwords = np.take(CountVectMNB.get_feature_names(), INTJ_class_prob_sorted[:20])
INTPwords


print("\nTop 20 ISFJ Words: ")
print(np.take(CountVectMNB.get_feature_names(), ISFJ_class_prob_sorted[:20]))
ISFJwords = np.take(CountVectMNB.get_feature_names(), ISFJ_class_prob_sorted[:20])
ISFJwords
print("\nTop 20 ISFP Words: ")
print(np.take(CountVectMNB.get_feature_names(), ISFP_class_prob_sorted[:20]))
ISFPwords = np.take(CountVectMNB.get_feature_names(), ISFP_class_prob_sorted[:20])
ISFPwords
print("\nTop 20 ISTJ Words: ")
print(np.take(CountVectMNB.get_feature_names(), ISTJ_class_prob_sorted[:20]))
ISTJwords = np.take(CountVectMNB.get_feature_names(), ISTJ_class_prob_sorted[:20])
ISTJwords
print("\nTop 20 ISTP Words: ")
print(np.take(CountVectMNB.get_feature_names(), ISTJ_class_prob_sorted[:20]))
ISTPwords = np.take(CountVectMNB.get_feature_names(), ISTJ_class_prob_sorted[:20])
ISTPwords

mbti_list = ["ENFJ", "ENFP", "ENTJ", "ENTP", "ESFJ", "ESFP", "ESTJ", "ESTP", "INFJ", "INFP", "INTJ", "INTP", "ISFJ", "ISFP", "ISTJ", "ISTP"]
import numpy as np
newarray1 = np.vstack((ENFJwords, ENFPwords, ENTJwords,ENTPwords, ESFJwords, ESFPwords,ESTJwords, ESTPwords, INFJwords, INFPwords, INTJwords, INTPwords, ISFJwords, ISFPwords, ISTJwords, ISTPwords)).T  
print(newarray1)
row_format ="{:<12}" * (len(mbti_list) + 1)
print(row_format.format("", *mbti_list))
for team, row in zip(mbti_list, newarray1):
    print(row_format.format(team, *row))


###############
## Rankings
###################
print("\n\nFOR ENFJ Words:")
FeatureRanks0=sorted(zip(mbtiModelMNB.coef_[0],CountVectMNB.get_feature_names()))
Ranks0=FeatureRanks0[-20:]
## Make them unique
Ranks0 = list(set(Ranks0))
print(Ranks0)

print("\n\nFOR ENFP Words:")
FeatureRanks1=sorted(zip(mbtiModelMNB.coef_[1],CountVectMNB.get_feature_names()))
Ranks1=FeatureRanks1[-20:]
## Make them unique
Ranks1 = list(set(Ranks1))
print(Ranks1)

print("\n\nFOR ENTJ Words:")
FeatureRanks2=sorted(zip(mbtiModelMNB.coef_[2],CountVectMNB.get_feature_names()))
Ranks2=FeatureRanks2[-20:]
## Make them unique
Ranks2 = list(set(Ranks2))
print(Ranks2)

print("\n\nFOR ENTP Words:")
FeatureRanks3=sorted(zip(mbtiModelMNB.coef_[3],CountVectMNB.get_feature_names()))
Ranks3 = FeatureRanks3[-20:]
## Make them unique
Ranks3 = list(set(Ranks3))
print(Ranks3)


print("\n\nFOR ESFJ Words:")
FeatureRanks4=sorted(zip(mbtiModelMNB.coef_[4],CountVectMNB.get_feature_names()))
Ranks4=FeatureRanks4[-20:]
## Make them unique
Ranks4 = list(set(Ranks4))
print(Ranks4)

print("\n\nFOR ESFP Words:")
FeatureRanks5=sorted(zip(mbtiModelMNB.coef_[5],CountVectMNB.get_feature_names()))
Ranks5=FeatureRanks5[-20:]
## Make them unique
Ranks5 = list(set(Ranks5))
print(Ranks5)

print("\n\nFOR ESTJ Words:")
FeatureRanks6=sorted(zip(mbtiModelMNB.coef_[6],CountVectMNB.get_feature_names()))
Ranks6=FeatureRanks6[-20:]
## Make them unique
Ranks6 = list(set(Ranks6))
print(Ranks6)

print("\n\nFOR ESTP Words:")
FeatureRanks7=sorted(zip(mbtiModelMNB.coef_[7],CountVectMNB.get_feature_names()))
Ranks7 = FeatureRanks7[-20:]
## Make them unique
Ranks7 = list(set(Ranks7))
print(Ranks7)


print("\n\nFOR INFJ Words:")
FeatureRanks8=sorted(zip(mbtiModelMNB.coef_[8],CountVectMNB.get_feature_names()))
Ranks8=FeatureRanks8[-20:]
## Make them unique
Ranks8 = list(set(Ranks8))
print(Ranks8)

print("\n\nFOR INFP Words:")
FeatureRanks9=sorted(zip(mbtiModelMNB.coef_[9],CountVectMNB.get_feature_names()))
Ranks9=FeatureRanks9[-20:]
## Make them unique
Ranks9 = list(set(Ranks9))
print(Ranks9)

print("\n\nFOR INTJ Words:")
FeatureRanks10=sorted(zip(mbtiModelMNB.coef_[10],CountVectMNB.get_feature_names()))
Ranks10=FeatureRanks10[-20:]
## Make them unique
Ranks10 = list(set(Ranks10))
print(Ranks10)

print("\n\nFOR INTP Words:")
FeatureRanks11=sorted(zip(mbtiModelMNB.coef_[11],CountVectMNB.get_feature_names()))
Ranks11 = FeatureRanks11[-20:]
## Make them unique
Ranks11 = list(set(Ranks11))
print(Ranks11)


print("\n\nFOR ISFJ Words:")
FeatureRanks12=sorted(zip(mbtiModelMNB.coef_[12],CountVectMNB.get_feature_names()))
Ranks12=FeatureRanks12[-20:]
## Make them unique
Ranks12 = list(set(Ranks12))
print(Ranks12)

print("\n\nFOR ISFP Words:")
FeatureRanks13=sorted(zip(mbtiModelMNB.coef_[13],CountVectMNB.get_feature_names()))
Ranks13=FeatureRanks13[-20:]
## Make them unique
Ranks13 = list(set(Ranks13))
print(Ranks13)

print("\n\nFOR ISTJ Words:")
FeatureRanks14=sorted(zip(mbtiModelMNB.coef_[14],CountVectMNB.get_feature_names()))
Ranks14=FeatureRanks14[-20:]
## Make them unique
Ranks14 = list(set(Ranks14))
print(Ranks14)

print("\n\nFOR ISTP Words:")
FeatureRanks15=sorted(zip(mbtiModelMNB.coef_[15],CountVectMNB.get_feature_names()))
Ranks15 = FeatureRanks15[-20:]
## Make them unique
Ranks15 = list(set(Ranks15))
print(Ranks15)


######################################################
## TFIDF #############################################
######################################################


from sklearn.feature_extraction.text import TfidfVectorizer
## Vectorize the text data for TFIDF
TfidfVectNB = TfidfVectorizer(input='filename',
                                    analyzer = 'word',
                                    #stop_words ='english',
                                    strip_accents = 'ascii',
                                    #tokenizer=textblob_tokenizer, #LemmaTokenizer(),
                                    stop_words = stop_words,
                                    token_pattern='(?u)[a-zA-Z]+',
                                    #r"\w+", #'(?u)r[^\w\s a-zA-Z]+',
                                    max_features = 1000000,
                                    decode_error = 'ignore'
                                    )

FinaltfidfDF = pd.DataFrame()

for R in ["ENFJ", "ENFP", "ENTJ", "ENTP", "ESFJ", "ESFP", "ESTJ", "ESTP", "INFJ", "INFP", "INTJ", "INTP", "ISFJ", "ISFP", "ISTJ", "ISTP"]:
    builderTF = R + "DFTF"
    print(builderTF)
    path = "C:\\Users\\GORGEOUS!\\Desktop\\2020-0405 IST Text Mining\\Project\\" + R
    
    FileList=[]
    for item in os.listdir(path):
        #print(path+ "\\" + item)
        next= path + "\\" + item
        FileList.append(next)  
        #print(FileList)
        
        X_TFIDFNB = TfidfVectNB.fit_transform(FileList)
        ColumnNamesTF = TfidfVectNB.get_feature_names()
        #print("Column names: ", ColumnNamesTF)

    #print(builderTF)
        
    builderTF = pd.DataFrame(X_TFIDFNB.toarray(), columns = ColumnNamesTF)
    ## Add column
    #print("Adding new column....")
    builderTF["label"] = R
    #print(builderTF)
              
    FinaltfidfDF = FinaltfidfDF.append(builderTF)
    #print(FinalmbtiDF.columns)
    print("\n\n TFIDF Vect")
    print(FinaltfidfDF)
    TfidfMatrix = FinaltfidfDF.values
    print(TfidfMatrix)


FinaltfidfDF1 = FinaltfidfDF

with open(DICTIONARY) as DICT:
    words = set(line.strip() for line in DICT)
    #print(words)    
    cols2 = [c for c in FinaltfidfDF1.columns if c.lower()[:] in words]
    FinaltfidfDF1 = FinaltfidfDF1[cols2]
  
list(FinaltfidfDF1.columns)
print("\n\n Remove words not in dictionary  from TFIDF DF")
FinaltfidfDF1.head()   
  
## Replace the NaN witH 0
FinaltfidfDF1 = FinaltfidfDF1.fillna(0)
#FinalbDF=FinalbDF.fillna(0)
print("\nTFIDF DF: ")  
print(FinaltfidfDF1)   


############# Create Train/Test for TFIDF  
from sklearn.model_selection import train_test_split
## Create Train/Test for TrainDF
TrainDFtfidf, TestDFtfidf = train_test_split(FinaltfidfDF1, test_size=0.2)
print(TrainDFtfidf.head())

print("\nThe TFIDF training set is:")
print(TrainDFtfidf)
print("\nThe TFIDF testing set is:")
print(TestDFtfidf)

## Save labels
TestLabelstfidf=TestDFtfidf["label"]
print(TestLabelstfidf)
## remove labels
TestDFtfidf = TestDFtfidf.drop(["label"], axis=1)
print(TestDFtfidf)

TrainLabelstfidf=TrainDFtfidf["label"]
print(TrainLabelstfidf)
TrainDFtfidf_nolabels=TrainDFtfidf.drop(["label"], axis=1)
print(TrainDFtfidf_nolabels) 

    
from sklearn. import MultinomialNB
mbtiModelTFIDF = MultinomialNB()
mbtiModelTFIDF.fit(TrainDFtfidf_nolabels, TrainDFtfidf)


## Prediction
PredictionTF = mbtiModelTFIDF.predict(TestDFtfidf)
print("\nThe prediction from TFIDF is:")
print(PredictionTF)
print("\nThe actual labels are:")
print(TestLabelstfidf)

## confusion matrix
from sklearn.metrics import confusion_matrix
cnf_matrixTF = confusion_matrix(TestLabelstfidf, PredictionTF)
print("\nThe TFIDF confusion matrix is:")
print(cnf_matrixTF)

print("\nThe TFIDF prediction probabilities:")
print(np.round(mbtiModelTFIDF.predict_proba(TestDFtfidf),2))

from sklearn.metrics import accuracy_score
print("\nTFIDF Accuracy score: ", accuracy_score(TestLabelstfidf, PredictionTF))

from sklearn.metrics import classification_report  
print(classification_report(TestLabelstfidf, PredictionTF))




#######################################################
### Bernoulli #########################################
#######################################################


CountVectBNB = CountVectorizer(input='filename',
                             analyzer = 'word',
                             #stop_words='english',
                             stop_words = stop_words,
                             token_pattern='(?u)[a-zA-Z]+',
                             #token_pattern=pattern,
                             #tokenizer=LemmaTokenizer(),
                             #strip_accents = 'unicode', 
                             max_features = 1000000,
                             decode_error = 'ignore',
                             binary=True
                             )

#Create  a new empty data frame for Bernolli 
FinalbDF = pd.DataFrame()

for R in ["ENFJ", "ENFP", "ENTJ", "ENTP", "ESFJ", "ESFP", "ESTJ", "ESTP", "INFJ", "INFP", "INTJ", "INTP", "ISFJ", "ISFP", "ISTJ", "ISTP"]:
   
    builderB = R + "DFB"
    print(builderB)
    path = "C:\\Users\\GORGEOUS!\\Desktop\\2020-0405 IST Text Mining\\Project\\" + R
    
    FileList=[]
    for item in os.listdir(path):
        #print(path+ "\\" + item)
        next= path + "\\" + item
        FileList.append(next)  
        #print(FileList)
        
        X_BNB = CountVectBNB.fit_transform(FileList)  
        ColumnNamesB = CountVectBNB.get_feature_names()
        #print("Column names: ", ColumnNamesB)

   #print(builderB)
           
    builderB= pd.DataFrame(X_BNB.toarray(),columns = ColumnNamesB)   
    ## Add column
    #print("Adding new column....")
    builderB["label"] = R
    #print(builderB)
               
    FinalbDF = FinalbDF.append(builderB)
    
    print("\n\n Bernoulli Vect")
    print(FinalbDF)
    #print(FinalmbtiDF.columns)
    
 
FinalbDF1 = FinalbDF

with open(DICTIONARY) as DICT:
    words = set(line.strip() for line in DICT)
    #print(words)    
    cols3 = [c for c in FinalbDF1.columns if c.lower()[:] in words]
    FinalbDF1 = FinalbDF1[cols3]
  
list(FinalbDF1.columns)
print("\n\n Remove words not in dictionary from Bernoulli DF")
FinalbDF1.head()   
  
## Replace the NaN witH 0
FinalbDF1 = FinalbDF1.fillna(0)
print("\nBINARY DF: ")
print(FinalbDF1)  


############# Create Train/Test for the binary dataframe
TrainDFB, TestDFB = train_test_split(FinalbDF1, test_size=0.3)
print(TrainDFB.head())

##-----------------------------------------------------------------
print("\nThe Bernoulli training set is:")
print(TrainDFB)
print("\nThe Bernoulli testing set is:")
print(TestDFB)

TrainLabelsB=TrainDFB["label"]
print(TrainLabelsB)
TrainDFB_nolabels=TrainDFB.drop(["label"], axis=1)
print(TrainDFB_nolabels)

TestLabelsB=TestDFB["label"]
print(TestLabelsB)
TestDFB_nolabels=TestDFB.drop(["label"], axis=1)
print(TestDFB_nolabels)



from sklearn.naive_bayes import BernoulliNB
mbtiModelBNB = BernoulliNB()
mbtiModelBNB.fit(TrainDFB_nolabels, TrainLabelsB)

## Prediction
PredictionB = mbtiModelBNB.predict(TestDFB_nolabels)
print("\nThe prediction from Bernoulli is:")
print(PredictionB)
print("\nThe actual labels are:")
print(TestLabelsB)

bn_matrix = confusion_matrix(TestLabelsB, mbtiModelBNB.predict(TestDFB_nolabels))
print("\nThe Bernoulli confusion matrix is:")
print(bn_matrix)

print("\nThe Bernoulli  prediction probabilities:")
print(np.round(mbtiModelBNB.predict_proba(TestDFB_nolabels),2))

from sklearn.metrics import accuracy_score
print("\nBernoulli Accuracy score: ", accuracy_score(TestLabelsB, PredictionB))

from sklearn.metrics import classification_report  
print(classification_report(TestLabelsB, PredictionB))

"""## Jonathan Ortiz Section:"""

# Commented out IPython magic to ensure Python compatibility.
# imports
import pandas as pd
import numpy as np
import seaborn as sns
from google.colab import files
import io
import matplotlib.pyplot as plt
import os
# %matplotlib inline
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import nltk
from nltk.corpus import stopwords
## For Stemming
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
import string

### this line of code allows your to choose the file to download
#uploaded = files.upload()

#### Store our Posts data into a dataframe
#df = pd.read_csv(io.BytesIO(uploaded['mbti_1.csv']))
#print(df)
dfx = df.posts
dfy = df.type
vectr=TfidfVectorizer(lowercase = True,
                        stop_words='english',
                        token_pattern=r"[a-zA-Z]+",
                        max_features=1000000,
                        decode_error='ignore')
X = vectr.fit_transform(dfx)
X.shape
X_train, x_test, Y_train, y_test = train_test_split(X, dfy, test_size=0.1, random_state=81)
print(X_train)

########################################
#### TFID Vectorizer
########################################
vectr=TfidfVectorizer(lowercase = True,
                        stop_words='english',
                        token_pattern=r"[a-zA-Z]+",
                        max_features=1000000,
                        decode_error='ignore')
X = vectr.fit_transform(dfx)
X.shape
X_train, x_test, Y_train, y_test = train_test_split(X, dfy, test_size=0.1, random_state=81)
print(X_train)
######################################
# Linear Kernel 
######################################
from sklearn.svm import LinearSVC
SVM_Model=LinearSVC(C=10)
SVM_Model.fit(X_train, Y_train)
MyPred=SVM_Model.predict(x_test)
print("SVM prediction:\n", MyPred)
print("Actual:")
print(y_test)

from sklearn.metrics import classification_report,  accuracy_score
print(accuracy_score(y_test, MyPred))
print(classification_report(y_test, MyPred))
from sklearn.metrics import confusion_matrix
SVM_matrix = confusion_matrix(y_test, MyPred)
print("\nThe confusion matrix is:")
print(SVM_matrix)
print("\n\n")
#########################################
## Support Vector Machine RBF Kernel #
#########################################
import sklearn.svm
SVM_Model2=sklearn.svm.SVC(C=10, kernel='rbf', 
                           verbose=True, gamma="auto")
SVM_Model2.fit(X_train, Y_train)
MyPred=SVM_Model2.predict(x_test)
print("SVM prediction:\n", MyPred)
print("Actual:")
print(y_test)

from sklearn.metrics import classification_report,  accuracy_score
print(accuracy_score(y_test, MyPred))
print(classification_report(y_test, MyPred))
from sklearn.metrics import confusion_matrix
SVM_matrix = confusion_matrix(y_test, MyPred)
print("\nThe confusion matrix is:")
print(SVM_matrix)
print("\n\n")

#########################################
## Support Vector Machine Poly Kernel #
#########################################
SVM_Model3=sklearn.svm.SVC(C=10, kernel='poly',degree=2,
                           gamma="auto", verbose=True)

SVM_Model3.fit(X_train, Y_train)
MyPred=SVM_Model3.predict(x_test)
print("SVM prediction:\n", MyPred)
print("Actual:")
print(y_test)

from sklearn.metrics import classification_report,  accuracy_score
print(accuracy_score(y_test, MyPred))
print(classification_report(y_test, MyPred))
from sklearn.metrics import confusion_matrix
SVM_matrix = confusion_matrix(y_test, MyPred)
print("\nThe confusion matrix is:")
print(SVM_matrix)
print("\n\n")

###################################
# CountVectorizer for SVM
###################################
vectr=CountVectorizer(lowercase = True,
                        stop_words='english',
                        token_pattern=r"[a-zA-Z]+",
                        max_features=1000000,
                        decode_error='ignore')
X = vectr.fit_transform(dfx)
X.shape
X_train, x_test, Y_train, y_test = train_test_split(X, dfy, test_size=0.1, random_state=81)
print(X_train)
######################################
# Linear Kernel 
######################################
from sklearn.svm import LinearSVC
SVM_Model=LinearSVC(C=10)
SVM_Model.fit(X_train, Y_train)
MyPred=SVM_Model.predict(x_test)
print("SVM prediction:\n", MyPred)
print("Actual:")
print(y_test)

from sklearn.metrics import classification_report,  accuracy_score
print(accuracy_score(y_test, MyPred))
print(classification_report(y_test, MyPred))
from sklearn.metrics import confusion_matrix
SVM_matrix = confusion_matrix(y_test, MyPred)
print("\nThe confusion matrix is:")
print(SVM_matrix)
print("\n\n")
#########################################
## Support Vector Machine RBF Kernel #
#########################################
import sklearn.svm
SVM_Model2=sklearn.svm.SVC(C=10, kernel='rbf', 
                           verbose=True, gamma="auto")
SVM_Model2.fit(X_train, Y_train)
MyPred=SVM_Model2.predict(x_test)
print("SVM prediction:\n", MyPred)
print("Actual:")
print(y_test)

from sklearn.metrics import classification_report,  accuracy_score
print(accuracy_score(y_test, MyPred))
print(classification_report(y_test, MyPred))
from sklearn.metrics import confusion_matrix
SVM_matrix = confusion_matrix(y_test, MyPred)
print("\nThe confusion matrix is:")
print(SVM_matrix)
print("\n\n")

#########################################
## Support Vector Machine Poly Kernel #
#########################################
SVM_Model3=sklearn.svm.SVC(C=10, kernel='poly',degree=2,
                           gamma="auto", verbose=True)

SVM_Model3.fit(X_train, Y_train)
MyPred=SVM_Model3.predict(x_test)
print("SVM prediction:\n", MyPred)
print("Actual:")
print(y_test)

from sklearn.metrics import classification_report,  accuracy_score
print(accuracy_score(y_test, MyPred))
print(classification_report(y_test, MyPred))
from sklearn.metrics import confusion_matrix
SVM_matrix = confusion_matrix(y_test, MyPred)
print("\nThe confusion matrix is:")
print(SVM_matrix)
print("\n\n")

###################################
# Boolean for SVM
###################################
vectr=CountVectorizer(lowercase = True,
                      binary = True,
                        stop_words='english',
                        token_pattern=r"[a-zA-Z]+",
                        max_features=1000000,
                        decode_error='ignore')
X = vectr.fit_transform(dfx)
X.shape
X_train, x_test, Y_train, y_test = train_test_split(X, dfy, test_size=0.1, random_state=81)
print(X_train)
######################################
# Linear Kernel 
######################################
from sklearn.svm import LinearSVC
SVM_Model=LinearSVC(C=10)
SVM_Model.fit(X_train, Y_train)
MyPred=SVM_Model.predict(x_test)
print("SVM prediction:\n", MyPred)
print("Actual:")
print(y_test)

from sklearn.metrics import classification_report,  accuracy_score
print(accuracy_score(y_test, MyPred))
print(classification_report(y_test, MyPred))
from sklearn.metrics import confusion_matrix
SVM_matrix = confusion_matrix(y_test, MyPred)
print("\nThe confusion matrix is:")
print(SVM_matrix)
print("\n\n")
#########################################
## Support Vector Machine RBF Kernel #
#########################################
SVM_Model2=sklearn.svm.SVC(C=10, kernel='rbf', 
                           verbose=True, gamma="auto")
SVM_Model2.fit(X_train, Y_train)
MyPred=SVM_Model2.predict(x_test)
print("SVM prediction:\n", MyPred)
print("Actual:")
print(y_test)

from sklearn.metrics import classification_report,  accuracy_score
print(accuracy_score(y_test, MyPred))
print(classification_report(y_test, MyPred))
from sklearn.metrics import confusion_matrix
SVM_matrix = confusion_matrix(y_test, MyPred)
print("\nThe confusion matrix is:")
print(SVM_matrix)
print("\n\n")

#########################################
## Support Vector Machine Poly Kernel #
#########################################
SVM_Model3=sklearn.svm.SVC(C=10, kernel='poly',degree=2,
                           gamma="auto", verbose=True)

SVM_Model3.fit(X_train, Y_train)
MyPred=SVM_Model3.predict(x_test)
print("SVM prediction:\n", MyPred)
print("Actual:")
print(y_test)

from sklearn.metrics import classification_report,  accuracy_score
print(accuracy_score(y_test, MyPred))
print(classification_report(y_test, MyPred))
from sklearn.metrics import confusion_matrix
SVM_matrix = confusion_matrix(y_test, MyPred)
print("\nThe confusion matrix is:")
print(SVM_matrix)
print("\n\n")

vectr=CountVectorizer(lowercase = True,
                        stop_words='english',
                        token_pattern=r"[a-zA-Z]+",
                        max_features=1000000,
                        decode_error='ignore')
k = 2
km = KMeans(n_clusters=k, init='random', n_init=3, random_state=0, verbose=False)
km.fit(vectr.fit_transform(dfx))
print("Top terms per cluster:")
order_centroids = km.cluster_centers_.argsort()[:, ::-1]
terms = vectr.get_feature_names()
for i in range(k):
    print("Cluster %d:" % i),
    for ind in order_centroids[i, :10]:
        print(' %s' % terms[ind]),
    print

vectr=CountVectorizer(lowercase = True,
                      binary = True,
                        stop_words='english',
                        token_pattern=r"[a-zA-Z]+",
                        max_features=1000000,
                        decode_error='ignore')
k = 2
km = KMeans(n_clusters=k, init='random', n_init=3, random_state=0, verbose=False)
km.fit(vectr.fit_transform(dfx))
print("Top terms per cluster:")
order_centroids = km.cluster_centers_.argsort()[:, ::-1]
terms = vectr.get_feature_names()
for i in range(k):
    print("Cluster %d:" % i),
    for ind in order_centroids[i, :10]:
        print(' %s' % terms[ind]),
    print

from sklearn.cluster import KMeans
vectr=TfidfVectorizer(lowercase = True,
                        stop_words= 'english',
                        token_pattern=r"[a-zA-Z]+",
                        max_features=1000000,
                        decode_error='ignore')
k = 2
km = KMeans(n_clusters=k, init='random', n_init=3, random_state=0, verbose=False)
km.fit(vectr.fit_transform(dfx))
print("Top terms per cluster:")
order_centroids = km.cluster_centers_.argsort()[:, ::-1]
terms = vectr.get_feature_names()
for i in range(k):
    print("Cluster %d:" % i),
    for ind in order_centroids[i, :10]:
        print(' %s' % terms[ind]),
    print

###########################
# LDA Topic Modeling 
###########################
# was just missing the import so added it- Neal
from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD

myvect=TfidfVectorizer(input= 'df',
                       stop_words='english',
                        token_pattern=r"[a-zA-Z]+",)

def print_topics(model, vectorizer, top_n=10):
    for idx, topic in enumerate(model.components_):
        print("Topic %d:" % (idx))
        print([(vectorizer.get_feature_names()[i], topic[i])
                        for i in topic.argsort()[:-top_n - 1:-1]])


Vect_DH = myvect.fit_transform(dfx)
ColumnNamesLDA_DH=myvect.get_feature_names()
CorpusDF_DH=pd.DataFrame(Vect_DH.toarray(),columns=ColumnNamesLDA_DH)
print(CorpusDF_DH)


lda_model_DH = LatentDirichletAllocation(n_components=2, max_iter=10, learning_method='online')
#lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online')
LDA_DH_Model = lda_model_DH.fit_transform(Vect_DH)

print("SIZE: ", LDA_DH_Model.shape)  # (NO_DOCUMENTS, NO_TOPICS)

## Print LDA using print function from above
print("LDA Model:")
print_topics(lda_model_DH, myvect)

!pip install pyLDAvis
import pyLDAvis.sklearn as LDAvis
import pyLDAvis
# google coalb needs it this way - Neal

import pyLDAvis
#pyLDAvis.enable_notebook() ## not using notebook
panel = LDAvis.prepare(lda_model_DH, Vect_DH, myvect, mds='tsne')
### !!!!!!! Important - you must interrupt and close the kernet in Spyder to end
## In other words - press the red square and then close the small red x to close
## the Console
# pyLDAvis.show(panel)
pyLDAvis.display(panel)

"""## Sandra Haskins Section:"""

# Imports

import os
import csv
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Upload csv file
from google.colab import files

uploaded = files.upload()

'''
MBTI_raw_df = df
MBTI_df = df
'''

# Load file into dataframe
import io
MBTI_df_raw = pd.read_csv(io.BytesIO(uploaded['mbti_1.csv']))
# df = MBTI_df_raw.copy()

# Look at source data
pd.set_option('display.max_colwidth',150)
print(MBTI_df_raw[50:60])

MBTI_df_raw.head()
print(MBTI_df_raw[0:15])

# Plot counts of MBTI Type
sns.set_style('whitegrid')
chart = sns.countplot(x='type', data=MBTI_df_raw, palette='Set2',
                      order=MBTI_df_raw['type'].value_counts().index)
chart.set_xticklabels(chart.get_xticklabels(),rotation=90)
chart.set(xlabel='MBTI Type', ylabel='Number of Individuals')
chart.set_title('MBTI Type vs Number of Indiviuals in Kaggle Dataset')
chart.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda y, loc:'{:,}'.format(int(y))))

#plt.savefig('MTBI_Type_Count.png', dpi=400)
#files.download('MTBI_Type_Count.png')

# Get hex-format colors in pallet Set2
import matplotlib

theme = plt.get_cmap('Set2')

for i in range(theme.N):
  rgb = theme(i)[:3]
  print(matplotlib.colors.rgb2hex(rgb))

# Pie Chart of Categories
my_labels = ['INFP','INFJ','INTP','INTJ','ENTP','ENFP','ISTP','ISFP','ENTJ','ISTJ','ENFJ','ISFJ','ESTP','ESFP','ESFJ','ESTJ']
sizes = [21.1, 16.9, 15.0, 12.6, 7.9, 7.8, 3.9, 3.1, 2.7, 2.4, 2.2, 1.9, 1.0, 0.6, 0.5, 0.4]

my_colors = ['#66c2a5','#fc8d62','#8da0cb','#e78ac3','#a6d854','#ffd92f','#e5c494','#b3b3b3']

#patches, texts = plt.pie(sizes, labels=my_labels, autopct='%1.1f%%', colors=my_colors, startangle=90)
patches, texts = plt.pie(sizes, colors=my_colors, startangle=90)
plt.legend(patches, labels=['%s, %1.1f %%' % (l,s) for l,s in zip(my_labels,sizes)], loc='right')
plt.axis('equal')
plt.subplots_adjust(left=0.0, bottom=0.1, right=1)
plt.title('Percentage of Users')

plt.savefig('MTBI_Type_Pie2.png', dpi=400)
files.download('MTBI_Type_Pie2.png')

#%% Clean Data

# Copy raw data into new dataframe
MBTI_df = MBTI_df_raw.copy()

# Remove URLs
MBTI_df['posts'] = MBTI_df['posts'].str.replace(r'(http?s|http)([a-zA-Z0-9:/.?=_-]+)', ' ')

# Remove all non alpha characters except ' and spaces between words
MBTI_df['posts'] = MBTI_df['posts'].str.replace(r'(?u)[^a-zA-Z \']+', ' ')

# Remove nonsense text strings (zzzzzzzzzzzzzzzz, aaaaaaaaaaaaaaand, etc..)
MBTI_df['posts'] = MBTI_df['posts'].str.replace(r'\w*(\w)\1{2,}\w*', ' ')

# Remove leading and trailing spaces
MBTI_df['posts'] = MBTI_df['posts'].str.strip()

# Removed first '
MBTI_df['posts'] = MBTI_df['posts'].str.replace(r'(^\')', '')

# Remove last '
MBTI_df['posts'] = MBTI_df['posts'].str.replace(r'(\'$)', '')

# Remove internal multiple white spaces
MBTI_df['posts'] = MBTI_df['posts'].str.replace(r'\s+', ' ')

# Remove leading and trailing spaces second pass
MBTI_df['posts'] = MBTI_df['posts'].str.strip()

MBTI_df.head(10)
MBTI_df.info()

>>> import nltk
  >>> nltk.download('wordnet')

#%% Lemmatize
lemmatizer = nltk.stem.WordNetLemmatizer()
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()

def lemmatize_text(text):
    ''' function to tokenize words, lemmatize text and put back
    into sentence form'''
    return ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])

MBTI_df['posts'] = MBTI_df['posts'].apply(lemmatize_text)

print(MBTI_df[0:10])

#%% Vectorize Data
VectTF = CountVectorizer(input='content', stop_words='english',
                         lowercase=True, min_df=5)

# Use CountVectorizer
x_Posts = VectTF.fit_transform(MBTI_df['posts'])
# print(x_Posts)

# Get Column Names
ColumnNames_Posts = VectTF.get_feature_names()
print(ColumnNames_Posts)

# Create dataframe of tokens
MBTI_tokens_df = pd.DataFrame(x_Posts.toarray(), columns=ColumnNames_Posts)
MBTI_tokens_df.head(15)
MBTI_tokens_df.info()

# Add labels to dataframe
MBTI_tokens_df.insert(loc=0, column='MBTI', value=MBTI_df.type)
#MBTI_tokens_df.to_csv(Path + r'\MBTI_Tokens.csv')

Unique_Labels = MBTI_tokens_df.MBTI.unique()
Unique_Labels.sort()
print(Unique_Labels)

#%% Create training and test sets
Train, Test =  train_test_split(MBTI_tokens_df, test_size=0.3)
Train.head()
Test.head()

# Save labels and removed from Train & Test dataframe
Train_Labels = Train['MBTI']
Train = Train.drop(['MBTI'], axis=1)

Test_Labels = Test['MBTI']
Test = Test.drop(['MBTI'], axis=1)

Train_Labels.head()
Test_Labels.head()

"""I ran KNN with two different weight settings, uniform and distance. Both provided the same results so distance is commented out."""

#%% KNN

# Run k = 1 through 25
k_range = range(1,26)
scores_k = {}
scores_list = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)
    knn.fit(Train, Train_Labels)
    print(k)
    Pred_Labels = knn.predict(Test)
    scores_k[k] = metrics.accuracy_score(Test_Labels, Pred_Labels)
    scores_list.append(metrics.accuracy_score(Test_Labels, Pred_Labels))

# Save results to csv file
'''
with open(Path +r'\KNN_Scores.csv', 'w') as myfile:
    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)
    wr.writerow(scores_list)
'''

# Load saved results to plot
#Scores_K_1_24 = pd.read_csv(Path + r'\KNN K_scores.csv')

Scores_K_1_24 = DataFrame(scores_list, columns=Accuracy)

# Plot K values vs accuracy
sns.set_style('whitegrid')
sns.lineplot(x='K', y='Accuracy', data=Scores_K_1_24)
plt.xlabel('Value for K for KNN')
plt.ylabel('Testing Accuracy')
plt.title('K Value vs Accuracy')
plt.savefig(Path + r'\K_Accuracy.png', dpi=400)

# Run classifier with K with best accuracy of 21
knn_u = KNeighborsClassifier(n_neighbors=21, weights='uniform', algorithm='auto',
                             n_jobs=-1)

knn_u.fit(Train, Train_Labels)
Pred_Labels_u = knn.predict(Test)

#knn_d = KNeighborsClassifier(n_neighbors=21, weights='distance', algorithm='auto',n_jobs=-1)

#knn_d.fit(Train, Train_Labels)
#Pred_Labels_d = knn.predict(Test)

# Accuracy of KNN
scores_u = metrics.accuracy_score(Test_Labels, Pred_Labels_u)
print(scores_u)

#scores_d = metrics.accuracy_score(Test_Labels, Pred_Labels_d)
#print(scores_d)

# Confusion Matrix
CM_knn_u = confusion_matrix(Test_Labels, Pred_Labels_u)
print(CM_knn_u)

sns.heatmap(CM_knn_u, cmap='BuGn', square=False, annot=True, fmt='d', cbar=False,
            xticklabels=Unique_Labels, yticklabels=Unique_Labels, linewidth=1,
            linecolor='white')
plt.xlabel('True Label')
plt.ylabel('Predicted Label')
plt.savefig(Path + r'\ConfusionMatrix_KNN_u.png', dpi=400)

#CM_knn_d = confusion_matrix(Test_Labels, Pred_Labels_d)
#print(CM_knn_d)

sns.heatmap(CM_knn_d, cmap='BuGn', square=False, annot=True, fmt='d', cbar=False,
            xticklabels=Unique_Labels, yticklabels=Unique_Labels, linewidth=1,
            linecolor='white')
#plt.xlabel('True Label')
#plt.ylabel('Predicted Label')
#plt.savefig(Path + r'\ConfusionMatrix_KNN_d.png', dpi=400)

# Scores
print(classification_report(Test_Labels, Pred_Labels_u, target_names=Unique_Labels))

#print(classification_report(Test_Labels, Pred_Labels_d, target_names=Unique_Labels))

#%% Random Forest

RF = RandomForestClassifier(n_estimators=100)

# Train
RF.fit(Train, Train_Labels)

# Predict
Pred_Labels_RF = RF.predict(Test)

# Confusion Matrix
CM_RF = confusion_matrix(Test_Labels, Pred_Labels_RF)
print(CM_RF)

sns.heatmap(CM_RF, cmap='BuGn', square=False, annot=True, fmt='d', cbar=False,
            xticklabels=Unique_Labels, yticklabels=Unique_Labels, linewidth=1,
            linecolor='white')
plt.xlabel('True Label')
plt.ylabel('Predicted Label')
plt.savefig(Path + r'\ConfusionMatrix_RF.png', dpi=400)

# Scores
print(classification_report(Test_Labels, Pred_Labels_RF, target_names=Unique_Labels))

# Feature Importance
features_RF = pd.Series(RF.feature_importances_,index=ColumnNames_Posts).sort_values(ascending=False)

print(features_RF[0:25])

"""## Neal Bates Section:"""

# setting up my own df
neal_df = df

# lets see the breakdown of the data
neal_df.shape

neal_df.head()

# Commented out IPython magic to ensure Python compatibility.
# lets look at the break down of labels.
import seaborn as sns
import matplotlib as plt
# %matplotlib inline

# lets look at the category distribution
from matplotlib import pyplot
a4_dims = (20.7,15.7)
fig, ax = pyplot.subplots(figsize=a4_dims)
ax = sns.countplot(x="type", data=neal_df)

# checking for NA
df.isnull().sum()

# setting up a test train split
import pandas as pd
import numpy as np
# splitting the data into test and train
from sklearn.model_selection import train_test_split

df_x = df.posts                                                    
df_y = df.type     


# dsplitting my dataset up
df['x'] = df['posts']

# Taking the lables (attend)
Y = df['type']

# Spliting into 90% for training set and 15 for testing set so we can see our accuracy
X_train, x_test, Y_train, y_test = train_test_split(df_x, df_y, test_size=0.1, random_state=1234)


print(len(X_train))                                                             # should be 7807
print(len(x_test))                                                              # should be 868

zz= pd.DataFrame(y_test)
from matplotlib import pyplot
a4_dims = (20.7,15.7)
fig, ax = pyplot.subplots(figsize=a4_dims)

ax = sns.countplot(x="type", data=zz)

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer                     # i know we have done the tranfomer, but I prefer the vctorizer.  it allows you to skip steps in your pipeline and of doing the contvectorizore and tfidf into one.  Pluse it has better params
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix

# EXPLORE
# EXTRACT FEATURES - DOCUMENT TERM MATRIX

count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(X_train)
X_train_counts.shape

#vectorizer = TfidfVectorizer(lowercase=True,  decode_error='ignore')

vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, max_features= 1000000, decode_error='ignore')
vectorizer.fit(X_train)

cls = MultinomialNB()
cls.fit(vectorizer.transform(X_train), Y_train)

from sklearn.metrics import classification_report,  accuracy_score
y_pred = cls.predict(vectorizer.transform(x_test))
print(accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# cuase I always learned to do this as a pipeline, doing this method to see if I got something wrong.# MODEL
# ALTERNATE METHOD FOR FREQUENCY DOCUMENTS
# BUILD A PIPELINE
# WILL DO THE COUNT AND INVERSE WITH CLASSIFICATION METHOD IN ONE CODE BLOCK

model = Pipeline([('vect', CountVectorizer()),
                      ('tfidf', TfidfTransformer()),
                      ('MNB', MultinomialNB()),       # n_iter chaned to n_iter_no_change
])

model = model.fit(X_train, Y_train)

# MODEL 
# COMPARE SVM CLASSIFIER AGAINST TEST SET

predicted = model.predict(x_test)
np.mean(predicted == y_test)

import sklearn
print(sklearn.metrics.confusion_matrix(y_test, predicted))

from sklearn.neural_network import MLPClassifier

# MODEL
# Build basic model using MLP Classifier

mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, alpha=1e-4, solver="adam",
                    verbose = 10, tol=1e-4, random_state=21, learning_rate_init=.1)

mlp.fit(vectorizer.transform(X_train), Y_train)

# getting the predicted scores for x test and seeing the precision/recall/f1-scores
y_pred6 = mlp.predict(vectorizer.transform(x_test))
print(accuracy_score(y_test, y_pred6))
print(classification_report(y_test, y_pred6))

import sklearn
print(sklearn.metrics.confusion_matrix(y_test, y_pred6))

#############################################################################################################
##############################################################################################################
# Build basic model using MLP Classifier

mlp1 = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, alpha=1e-4, solver="sgd",
                    verbose = 10, tol=1e-4, random_state=21, learning_rate_init=.1)

mlp1.fit(vectorizer.transform(X_train), Y_train)

# getting the predicted scores for x test and seeing the precision/recall/f1-scores
y_pred8 = mlp1.predict(vectorizer.transform(x_test))
print(accuracy_score(y_test, y_pred8))
print(classification_report(y_test, y_pred8))

#############################################################################################################
##############################################################################################################
# Build basic model using MLP Classifier

mlp2 = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, alpha=1e-4, solver="lbfgs",
                    verbose = 10, tol=1e-4, random_state=21, learning_rate_init=.1)

mlp2.fit(vectorizer.transform(X_train), Y_train)

# getting the predicted scores for x test and seeing the precision/recall/f1-scores
y_pred9 = mlp2.predict(vectorizer.transform(x_test))
print(accuracy_score(y_test, y_pred9))
print(classification_report(y_test, y_pred9))

# Build basic model using MLP Classifier

mlp3 = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, alpha=1e-4, solver="lbfgs",
                    verbose = 10, tol=1e-4, random_state=21, learning_rate_init=.1, activation='tanh')

mlp3.fit(vectorizer.transform(X_train), Y_train)

# getting the predicted scores for x test and seeing the precision/recall/f1-scores
y_pred9 = mlp3.predict(vectorizer.transform(x_test))
print(accuracy_score(y_test, y_pred9))
print(classification_report(y_test, y_pred9))

#############################################################################################################
##############################################################################################################
# Build basic model using MLP Classifier

mlp4 = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, alpha=1e-4, solver="sgd",
                    verbose = 10, tol=1e-4, random_state=21, learning_rate_init=.1, activation='tanh')

mlp4.fit(vectorizer.transform(X_train), Y_train)

# getting the predicted scores for x test and seeing the precision/recall/f1-scores
y_pred99 = mlp4.predict(vectorizer.transform(x_test))
print(accuracy_score(y_test, y_pred99))
print(classification_report(y_test, y_pred99))

"""## Prediciton Method:"""

#############################################################################################################
##############################################################################################################
# Build basic model using MLP Classifier

mlp5 = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, alpha=1e-4, solver="sgd",
                    verbose = 10, tol=1e-4, random_state=21, learning_rate_init=.15, activation='tanh')

mlp5.fit(vectorizer.transform(X_train), Y_train)

# getting the predicted scores for x test and seeing the precision/recall/f1-scores
y_pred88= mlp5.predict(vectorizer.transform(x_test))
print(accuracy_score(y_test, y_pred88))
print(classification_report(y_test, y_pred88))

#############################################################################################################
##############################################################################################################
# Build basic model using MLP Classifier

mlp5 = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, alpha=1e-4, solver="sgd",
                    verbose = 20, tol=1e-4, random_state=21, learning_rate_init=.25, activation='tanh')

mlp5.fit(vectorizer.transform(X_train), Y_train)

# getting the predicted scores for x test and seeing the precision/recall/f1-scores
y_pred88= mlp5.predict(vectorizer.transform(x_test))
print(accuracy_score(y_test, y_pred88))
print(classification_report(y_test, y_pred88))

import sklearn
print(sklearn.metrics.confusion_matrix(y_test, y_pred88))

# Commented out IPython magic to ensure Python compatibility.
ww = set(zz.type)

# lets look at the break down of labels.
import seaborn as sns
import matplotlib as plt
# %matplotlib inline

# lets look at the category distribution
from matplotlib import pyplot
from matplotlib.pyplot import figure
figure(num=None, figsize=(10, 10), dpi=90, facecolor='w', edgecolor='k')


mat = confusion_matrix(y_pred88, zz)
ax = sns.heatmap(mat.T, square=True, annot=True, fmt='d',
            cbar=False, xticklabels=ww,
            yticklabels= ww)

"""# so at this point it is looking pretty similar to last graph example but with less score.
## svm out preformed the niavebayes, in this account.  But instead of showing the differnce model's let's show how to predict.  
### This is also where the youtube video i get my code from for the prediciton function(https://www.youtube.com/watch?v=l3dZ6ZNFjo0)

def predict_category(s, train=X_train, model=mlp5):
  pred = model.predict(vectorizer.transform([s]))
  return print(pred)

## WaLL Post
"""

txt = input('Please put in your text: ')
print(txt)
type(txt)
txt_series = pd.Series(txt)

"""anym wall post"""

y_pre_zz = mlp5.predict(vectorizer.transform(txt_series))
y_pre_zz

"""anym wall post"""

txt = input('Please put in your text: ')
print(txt)
type(txt)
txt_series = pd.Series(txt)
y_pre_zz = mlp5.predict(vectorizer.transform(txt_series))
y_pre_zz

"""anym wall post"""

txt = input('Please put in your text: ')
print(txt)
type(txt)
txt_series = pd.Series(txt)
y_pre_zz = mlp5.predict(vectorizer.transform(txt_series))
y_pre_zz

txt = input('Please put in your text: ')
print(txt)
type(txt)
txt_series = pd.Series(txt)
y_pre_zz = mlp5.predict(vectorizer.transform(txt_series))
y_pre_zz

"""Trump"""

txt = input('Please put in your text: ')
print(txt)
type(txt)
txt_series = pd.Series(txt)
y_pre_zz = mlp5.predict(vectorizer.transform(txt_series))
y_pre_zz

"""Billie Eilish"""

txt = input('Please put in your text: ')
print(txt)
type(txt)
txt_series = pd.Series(txt)
y_pre_zz = mlp5.predict(vectorizer.transform(txt_series))
y_pre_zz

"""Elon Musk"""

txt = input('Please put in your text: ')
print(txt)
type(txt)
txt_series = pd.Series(txt)
y_pre_zz = mlp5.predict(vectorizer.transform(txt_series))
y_pre_zz

"""Noam Chomsky

Do not let your fire go out, spark by irreplaceable spark in the hopeless swamps of the not-quite, the not-yet, and the not-at-all. Do not let the hero in your soul perish in lonely frustration for the life you deserved and have never been able to reach. The world you desire can be won. It exists.. it is real.. it is possible.. it's yours.
"""

txt = input('Please put in your text: ')
print(txt)
type(txt)
txt_series = pd.Series(txt)
y_pre_zz = mlp5.predict(vectorizer.transform(txt_series))
y_pre_zz

txt = input('Please put in your text: ')
print(txt)
type(txt)
txt_series = pd.Series(txt)
y_pre_zz = mlp5.predict(vectorizer.transform(txt_series))
y_pre_zz

"""Ayn Rand

## Twitter API Tweets
"""

tweet = pd.read_excel('Tweets_With_Type.xlsx', index_col=0)  
tweet

y_pre_zz = mlp5.predict(vectorizer.transform(tweet['Original Post']))
y_pre_zz



"""## Try for yourself!"""

txt = input('Please put in your text: ')
print(txt)
type(txt)
txt_series = pd.Series(txt)
y_pre_zz = mlp5.predict(vectorizer.transform(txt_series))
y_pre_zz